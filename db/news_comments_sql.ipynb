{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Spark, S3, Redshift와의 연동"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "# 원하는 기사 제목\n",
        "article_title = \"\"\n",
        "\n",
        "# conf 설정\n",
        "conf = SparkConf()\n",
        "conf.set(\"spark.hadoop.fs.s3a.access.key\", \"access_key\")\n",
        "conf.set(\"spark.hadoop.fs.s3a.secret.key\", \"secret_key\")\n",
        "conf.setAll([('spark.driver.extraClassPath', '/skybluelee/spark3/jars/hadoop-aws-3.3.1.jar:/skybluelee/spark3/jars/aws-java-sdk-bundle-1.11.901.jar:/skybluelee/spark3/jars/RedshiftJDBC4-1.2.1.1001.jar:/skybluelee/spark3/jars/RedshiftJDBC42-no-awssdk-1.2.36.1060.jar')])\n",
        "# hadoop-aws, aws-java-sdk-bundle은 s3 <-> spark()\n",
        "# RedshiftJDBC4, RedshiftJDBC42-no-awssdk는 spark <-> redshift\n",
        "conf.set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
        "conf.set(\"fs.s3a.aws.credentials.provider\", \"com.amazonaws.auth.profile.ProfileCredentialsProvider\")\n",
        "conf.set('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider')\n",
        "conf.set(\"spark.hadoop.fs.s3a.endpoint\", \"s3.ap-northeast-1.amazonaws.com\")\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .config(conf=conf) \\\n",
        "    .appName(\"Spark\") \\\n",
        "    .getOrCreate()\n",
        "spark.sparkContext.setSystemProperty(\"com.amazonaws.services.s3.enableV4\", \"true\")\n",
        "\n",
        "df1 = spark.read \\\n",
        "        .option(\"header\", \"true\") \\\n",
        "        .csv(\"s3a://news-comments/comments_\" + article_title)             \n",
        "        \n",
        "df1 = df1.withColumn('good', col('good').cast(IntegerType()))\\\n",
        "         .withColumn('bad', col('bad').cast(IntegerType()))\\\n",
        "         .withColumn(\"timestamp\",to_timestamp(\"timestamp\"))\\\n",
        "         .withColumn(\"written_time\",to_timestamp(\"written_time\")) \\\n",
        "         .withColumn('DiffInMinutes', round((unix_timestamp(\"timestamp\") - unix_timestamp('written_time'))/60, 0))\n",
        "# spark로 df를 읽는 경우 전부 string으로 읽기 때문에 형 변환         \n",
        "\n",
        "df1.printSchema()      \n",
        "df1.show()\n",
        "df1.createOrReplaceTempView(\"comments\")\n",
        "\n",
        "df2 = spark.read \\\n",
        "        .option(\"header\", \"true\") \\\n",
        "        .csv(\"s3a://news-comments/user_distribution_\" + article_title)\n",
        "\n",
        "df2 = df2.withColumn('total', col('total').cast(IntegerType()))\\\n",
        "         .withColumn('self_removed', col('self_removed').cast(IntegerType()))\\\n",
        "         .withColumn('auto_removed', col('auto_removed').cast(IntegerType()))\\\n",
        "         .withColumn('male', col('male').cast(IntegerType()))\\\n",
        "         .withColumn('female', col('female').cast(IntegerType()))\\\n",
        "         .withColumn('age_10', col('age_10').cast(IntegerType()))\\\n",
        "         .withColumn('age_20', col('age_20').cast(IntegerType()))\\\n",
        "         .withColumn('age_30', col('age_30').cast(IntegerType()))\\\n",
        "         .withColumn('age_40', col('age_40').cast(IntegerType()))\\\n",
        "         .withColumn('age_50', col('age_50').cast(IntegerType()))\\\n",
        "         .withColumn('age_60', col('age_60').cast(IntegerType()))\\\n",
        "         .withColumn(\"timestamp\",to_timestamp(\"timestamp\"))     \n",
        "df2.printSchema()      \n",
        "df2.show()\n",
        "df2.createOrReplaceTempView(\"users\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SQL Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "spark.sql(\"\"\" WITH temp AS(\n",
        "                   SELECT comment, ROW_NUMBER() OVER (PARTITION BY timestamp ORDER BY good DESC) AS goodNo, timestamp, good, written_time, bad, DiffInMinutes,\n",
        "                          total, male, female, age_10 ,age_20, age_30, age_40, age_50, age_60\n",
        "                   FROM   comments C\n",
        "                   JOIN users U USING(timestamp)\n",
        "              )\n",
        "              SELECT comment, good, bad, timestamp, written_time, DiffInMinutes, total, ROUND(good/total, 3) AS good_rate, ROUND(bad/total, 3) AS bad_rate, male, female, age_10 ,age_20, age_30, age_40, age_50, age_60\n",
        "              FROM   temp\n",
        "              WHERE  goodNo < 6\"\"\") \\\n",
        "     .show(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "# 기사 입력 시간과 데이터 수집 시간 사이에 관계가 있을 것이라 생각하고 사용\n",
        "df_test = df1.withColumn('DiffInMinutes', round((unix_timestamp(\"timestamp\") - unix_timestamp('written_time'))/60, 0))\n",
        "\n",
        "df_test.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "# 각 시간대별로 좋아요 상위 10개의 댓글 수집\n",
        "df_redshift_test= spark.sql(\"\"\" WITH temp AS(\n",
        "                   SELECT C.title, comment, ROW_NUMBER() OVER (PARTITION BY timestamp ORDER BY good DESC) AS goodNo, timestamp, good, written_time, bad, DiffInMinutes,\n",
        "                          total, male, female, age_10 ,age_20, age_30, age_40, age_50, age_60\n",
        "                   FROM   comments C\n",
        "                   JOIN users U USING(timestamp)\n",
        "              )\n",
        "              SELECT title, comment, good, bad, timestamp, written_time, DiffInMinutes, total, ROUND(good/total, 3) AS good_rate, ROUND(bad/total, 3) AS bad_rate, male, female, age_10 ,age_20, age_30, age_40, age_50, age_60\n",
        "              FROM   temp\n",
        "              WHERE  goodNo < 11\"\"\") "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Redshift Upload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "# redshift upload\n",
        "df_redshift_test.write.format('jdbc').options(\n",
        "      url='<jdbc url>',\t  \n",
        "      driver='com.amazon.redshift.jdbc42.Driver',\n",
        "      dbtable='<schema>.<table>',\n",
        "      user='<userid>',\n",
        "      password='<password>').mode('overwrite').save() "
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": [
        "%pyspark\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "name": "news_comments_analysis"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
