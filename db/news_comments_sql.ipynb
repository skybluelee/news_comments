{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "# 원하는 기사 제목\n",
        "article_title = \"\"\n",
        "\n",
        "# conf 설정\n",
        "conf = SparkConf()\n",
        "conf.set(\"spark.hadoop.fs.s3a.access.key\", \"access_key\")\n",
        "conf.set(\"spark.hadoop.fs.s3a.secret.key\", \"secret_key\")\n",
        "conf.setAll([('spark.driver.extraClassPath', '/skybluelee/spark3/jars/hadoop-aws-3.3.1.jar:/skybluelee/spark3/jars/aws-java-sdk-bundle-1.11.901.jar:/skybluelee/spark3/jars/RedshiftJDBC4-1.2.1.1001.jar:/skybluelee/spark3/jars/RedshiftJDBC42-no-awssdk-1.2.36.1060.jar')])\n",
        "# hadoop-aws, aws-java-sdk-bundle은 s3 <-> spark()\n",
        "# RedshiftJDBC4, RedshiftJDBC42-no-awssdk는 spark <-> redshift\n",
        "conf.set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
        "conf.set(\"fs.s3a.aws.credentials.provider\", \"com.amazonaws.auth.profile.ProfileCredentialsProvider\")\n",
        "conf.set('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider')\n",
        "conf.set(\"spark.hadoop.fs.s3a.endpoint\", \"s3.ap-northeast-1.amazonaws.com\")\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .config(conf=conf) \\\n",
        "    .appName(\"Spark\") \\\n",
        "    .getOrCreate()\n",
        "spark.sparkContext.setSystemProperty(\"com.amazonaws.services.s3.enableV4\", \"true\")\n",
        "\n",
        "df1 = spark.read \\\n",
        "        .option(\"header\", \"true\") \\\n",
        "        .csv(\"s3a://news-comments/comments_\" + article_title)             \n",
        "        \n",
        "df1 = df1.withColumn('good', col('good').cast(IntegerType()))\\\n",
        "         .withColumn('bad', col('bad').cast(IntegerType()))\\\n",
        "         .withColumn(\"timestamp\",to_timestamp(\"timestamp\"))\\\n",
        "         .withColumn(\"written_time\",to_timestamp(\"written_time\")) \\\n",
        "         .withColumn('DiffInMinutes', round((unix_timestamp(\"timestamp\") - unix_timestamp('written_time'))/60, 0))\n",
        "# spark로 df를 읽는 경우 전부 string으로 읽기 때문에 형 변환         \n",
        "\n",
        "df1.printSchema()      \n",
        "df1.show()\n",
        "df1.createOrReplaceTempView(\"comments\")\n",
        "\n",
        "df2 = spark.read \\\n",
        "        .option(\"header\", \"true\") \\\n",
        "        .csv(\"s3a://news-comments/user_distribution_\" + article_title)\n",
        "\n",
        "df2 = df2.withColumn('total', col('total').cast(IntegerType()))\\\n",
        "         .withColumn('self_removed', col('self_removed').cast(IntegerType()))\\\n",
        "         .withColumn('auto_removed', col('auto_removed').cast(IntegerType()))\\\n",
        "         .withColumn('male', col('male').cast(IntegerType()))\\\n",
        "         .withColumn('female', col('female').cast(IntegerType()))\\\n",
        "         .withColumn('age_10', col('age_10').cast(IntegerType()))\\\n",
        "         .withColumn('age_20', col('age_20').cast(IntegerType()))\\\n",
        "         .withColumn('age_30', col('age_30').cast(IntegerType()))\\\n",
        "         .withColumn('age_40', col('age_40').cast(IntegerType()))\\\n",
        "         .withColumn('age_50', col('age_50').cast(IntegerType()))\\\n",
        "         .withColumn('age_60', col('age_60').cast(IntegerType()))\\\n",
        "         .withColumn(\"timestamp\",to_timestamp(\"timestamp\"))     \n",
        "df2.printSchema()      \n",
        "df2.show()\n",
        "df2.createOrReplaceTempView(\"users\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "spark.sql(\"\"\" WITH temp AS(\n",
        "                   SELECT comment, ROW_NUMBER() OVER (PARTITION BY timestamp ORDER BY good DESC) AS goodNo, timestamp, good, written_time, bad, DiffInMinutes,\n",
        "                          total, male, female, age_10 ,age_20, age_30, age_40, age_50, age_60\n",
        "                   FROM   comments C\n",
        "                   JOIN users U USING(timestamp)\n",
        "              )\n",
        "              SELECT comment, good, bad, timestamp, written_time, DiffInMinutes, total, ROUND(good/total, 3) AS good_rate, ROUND(bad/total, 3) AS bad_rate, male, female, age_10 ,age_20, age_30, age_40, age_50, age_60\n",
        "              FROM   temp\n",
        "              WHERE  goodNo < 6\"\"\") \\\n",
        "     .show(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "# 기사 입력 시간과 데이터 수집 시간 사이에 관계가 있을 것이라 생각하고 사용\n",
        "df_test = df1.withColumn('DiffInMinutes', round((unix_timestamp(\"timestamp\") - unix_timestamp('written_time'))/60, 0))\n",
        "\n",
        "df_test.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "# 각 시간대별로 좋아요 상위 10개의 댓글 수집\n",
        "df_redshift_test= spark.sql(\"\"\" WITH temp AS(\n",
        "                   SELECT C.title, comment, ROW_NUMBER() OVER (PARTITION BY timestamp ORDER BY good DESC) AS goodNo, timestamp, good, written_time, bad, DiffInMinutes,\n",
        "                          total, male, female, age_10 ,age_20, age_30, age_40, age_50, age_60\n",
        "                   FROM   comments C\n",
        "                   JOIN users U USING(timestamp)\n",
        "              )\n",
        "              SELECT title, comment, good, bad, timestamp, written_time, DiffInMinutes, total, ROUND(good/total, 3) AS good_rate, ROUND(bad/total, 3) AS bad_rate, male, female, age_10 ,age_20, age_30, age_40, age_50, age_60\n",
        "              FROM   temp\n",
        "              WHERE  goodNo < 11\"\"\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "# redshift upload\n",
        "df_redshift_test.write.format('jdbc').options(\n",
        "      url='jdbc:redshift://news-comments.235050266187.ap-northeast-1.redshift-serverless.amazonaws.com:5439/dev',\t  \n",
        "      driver='com.amazon.redshift.jdbc42.Driver',\n",
        "      dbtable='news_comments.comment_analysis',\n",
        "      user='skybluelee',\n",
        "      password='Kusdk3197.').mode('overwrite').save() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "#redshift upload\n",
        "df_redshift_test= spark.sql(\"\"\" WITH temp AS(\n",
        "                   SELECT C.title, comment, ROW_NUMBER() OVER (PARTITION BY timestamp ORDER BY good DESC) AS goodNo, timestamp, good, written_time, bad, DiffInMinutes,\n",
        "                          total, male, female, age_10 ,age_20, age_30, age_40, age_50, age_60\n",
        "                   FROM   comments C\n",
        "                   JOIN users U USING(timestamp)\n",
        "              )\n",
        "              SELECT title, comment, good, bad, timestamp, written_time, DiffInMinutes, total, ROUND(good/total, 3) AS good_rate, ROUND(bad/total, 3) AS bad_rate, male, female, age_10 ,age_20, age_30, age_40, age_50, age_60\n",
        "              FROM   temp\n",
        "              WHERE  goodNo < 6\"\"\") \n",
        "df = df_redshift_test.withColumn('comment', col('comment').alias(\"comment\", metadata={\"maxlength\":2048}))\n",
        "\n",
        "df_redshift_test.printSchema()\n",
        "df.printSchema()\n",
        "df_pandas = df_redshift_test.toPandas()\n",
        "              \n",
        "df.write.format('jdbc').options(\n",
        "      url='jdbc:redshift://news-comments.235050266187.ap-northeast-1.redshift-serverless.amazonaws.com:5439/dev',\t  \n",
        "      driver='com.amazon.redshift.jdbc42.Driver',\n",
        "      dbtable='news_comments.comment_analysis',\n",
        "      user='skybluelee',\n",
        "      password='Kusdk3197.').mode('overwrite').save()               "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "# S3 upload\n",
        "df_redshift_test.write \\\n",
        " .option(\"header\",\"true\") \\\n",
        " .option(\"encoding\", \"UTF-8\") \\\n",
        " .mode(\"overwrite\") \\\n",
        " .csv(\"s3a://news-comments/test\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "import org.apache.spark.sql.types.MetadataBuilder\n",
        "\n",
        "// Specify the custom width of each column\n",
        "val columnTypeMap = Map(\n",
        "  \"comment\" -> \"VARCHAR(2048)\"\n",
        ")\n",
        "\n",
        "var df = spark.sql(\"\"\" WITH temp AS(\n",
        "                   SELECT C.title, comment, ROW_NUMBER() OVER (PARTITION BY timestamp ORDER BY good DESC) AS goodNo, timestamp, good, written_time, bad, DiffInMinutes,\n",
        "                          total, male, female, age_10 ,age_20, age_30, age_40, age_50, age_60\n",
        "                   FROM   comments C\n",
        "                   JOIN users U USING(timestamp)\n",
        "              )\n",
        "              SELECT title, comment, good, bad, timestamp, written_time, DiffInMinutes, total, ROUND(good/total, 3) AS good_rate, ROUND(bad/total, 3) AS bad_rate, male, female, age_10 ,age_20, age_30, age_40, age_50, age_60\n",
        "              FROM   temp\n",
        "              WHERE  goodNo < 6\"\"\") \n",
        "             \n",
        "// Apply each column metadata customization\n",
        "columnTypeMap.foreach { case (colName, colType) =>\n",
        "  val metadata = new MetadataBuilder().putString(\"redshift_type\", colType).build()\n",
        "  df = df.withColumn(colName, df(colName).as(colName, metadata))\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "df.write.format('jdbc').options(\n",
        "      url='jdbc:redshift://news-comments.235050266187.ap-northeast-1.redshift-serverless.amazonaws.com:5439/dev',\t  \n",
        "      driver='com.amazon.redshift.jdbc42.Driver',\n",
        "      dbtable='news_comments.comment_analysis',\n",
        "      user='skybluelee',\n",
        "      password='Kusdk3197.').mode('overwrite').save()   "
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": [
        "%pyspark\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    },
    "name": "news_comments_analysis"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
